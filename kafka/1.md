# **Kafka 0.9.0 Documentation**
---
# 1 시작하기
## 1.1 소개
카프카는 분산되어있고, 파티션 기반이며, 복제기능이 있는 커밋 로그 서비스입니다.
카프카는 독특한 디자인의 메시징 시스템 기능을 제공합니다.<br>

이는 무엇을 의미하는 걸까요?<br>

먼저 몇가지 메시징의 의미를 되짚어 봅시다.

- 카프카는 메시지 피드들을 *토픽* 이라 불리는 카테고리 안에 담습니다.
- 카프카 토픽 *프로듀서* 에 메시지를 발행(publish)할 프로세스를 호출합니다.
- 토픽을 구독(subscribe)하고, 구독한 메시지 피드를 처리할 *컨슈머* 프로세스를 호출합니다.
- 카프카는 *브로커* 라 불리는 하나 이상의 서버 클러스터로 동작합니다.

즉 하이레벨에서 보면, 프로듀서는 네트워크를 통해 카프카 클러스터에 메시지를 보내고 클러스터는 이를 컨슈머들에게 전달합니다. 이렇게요:<br>

![producer consumer](image/producer_consumer.png)

클라이언트와 서버 사이의 커뮤니케이션은 *TCP 프로토콜* 을 사용하는데 적합한(agnostic) 간결하고, 높은 성능의 언어로 처리됩니다.
카프카는 자바 클라이언트를 제공하기는 하지만, 클라이언트에 다른 언어들도 사용할 수 있습니다.

### 토픽과 로그
카프카의 토픽에 대해서 알아봅시다.<br>

토픽은 보내지는 메시지의 카테고리 또는 피드 이름입니다.
카프카 클러스터는 각 토픽 안에, 아래 그림처럼 파티션된 로그를 가지고 있습니다:<br>

![log anatomy](image/log_anatomy.png)

각 파티션은 계속해서 추가되는 메시지들의 나열(이를 커밋 로그라고 한다)이며, 파티션 사이에는 변하지 않는 순서가 있습니다.
파티션들 속 메시지들은 서로를 구별하기 위한 순차적인 숫자 id를 가지고 있으며, 이를 *오프셋* 이라 부릅니다.<br>

카프카 클러스터는 정해진 기간동안 -이미 소비되었는지 여부와 상관 없이-보내진 모든 메시지들을 유지합니다.
메시지 유지기간을 이틀이라 한다면, 메시지가 보내진 이후의 이틀 동안 소비될 수 있도록 유지되다가, 그 이후에 스토리지에서 폐기됩니다.
카프카의 성능은 유지할 데이터의 크기와 관계가 없는데, 실제로 데이터 크기에 따른 성능이 일정하기 때문입니다.<br>

컨슈머 이전에 유지되는 메타데이터는 "오프셋"이라 불리는 로그 컨슈머의 위치 뿐입니다.
이 오프셋은 컨슈머에 의해 조정됩니다:보통 컨슈머는 오프셋에 따라 일정하게 메시지를 읽어 나가지만, 이 위치는 컨슈머에 의해 조정되어 원래 순서와 다르게 소비될 수도 있습니다.
예를 들어, 한 컨슈머는 메시지를 다시 처리하기위해 오프셋을 이전의 값으로 리셋할 수도 있습니다.<br>

이는 카프카의 컨슈머가 매우 저비용이며, 클러스터나 다른 컨슈머들에게 영향을 주지 않고도 운영될 수 있음을 의미합니다.
예를 들어, 기존의 컨슈머가 소비하고 있던 것들에 변화를 주지 않고도 어떤 토픽의 컨텐츠에 "tail" 명령을 사용할 수 있습니다.<br>

로그의 파티션들은 다양한 용도를 제공합니다.
먼저, 로그가 하나의 서버에 딱 맞는 크기 이상으로 확장할수 있도록 허용합니다.
각 파티션은 자신이 속한 서버의 크기보다 작아야 하지만, 한 토픽안에 다수의 파티션이 존재할 수 있어 카프카는 서버의 크기보다 많은 양의 데이터를 처리할 수 있습니다.
둘째로, 파티션들은 병렬화의 기본 단위로 동작합니다.

### 분배
로그의 파티션들은 카프카 클러스터에 공유되어 각 서버들에게 분배됩니다.
각 파티션은 결함이 발생할 것에 대비해서 몇 개의 서버들로 복제됩니다.<br>

각 파티션은 "리더"역할을 하는 하나의 서버와 "팔로워"역할을 하는 0개 이상의 서버를 갖습니다.
리더는 팔로워가 수동적으로 리더를 복제하는 동안 모든 읽기 쓰기 요청을 처리합니다.
만약 리더에 문제가 생긴다면, 팔로워중 하나가 자동적으로 새로운 리더가 됩니다.
각각의 서버들은 어떤 파티션에 대해서는 리더의 역할을 하기도 팔로워의 역할을 하며 클러스터 안에서 균일한 부하가 발생하도록 합니다.

### 프로듀서
프로듀서는 데이터를 특정 토픽에 선택하여 발행합니다.
그 프로듀서는 어떤 메시지가 해당 토픽 속 어떤 파티션으로 배치할지 선택할 책임이 있습니다.
이는 로드 밸런싱을 위해 간단하게 라운드 로빈으로 구현될 수도 있고 (키-메시지 기반이라 불라는) 몇 개의 시멘틱 파티션 기능을 통해 구현될 수 있습니다.
More on the use of partitioning in a second.

### 컨슈머
메시징은 보통 [큐잉](http://en.wikipedia.org/wiki/Message_queue)과 [발행-구독](http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern) 두 개의 모델을 가지고 있습니다.
큐에서는, 컨슈머 풀은 서버로부터 메시지를 읽고, 각 메시지들은 컨슈머 풀 중 한 곳으로 보내집니다;
발행-구동에서는, 메시지는 모든 컨슈머들에게 브로드캐스트 됩니다.
카프카는 두 모델을 일반화한 한 하나의 추상적인 컨슈머(컨슈머 그룹)를 제공합니다.<br>

컨슈머들은 컨슈머 그룹의 이름으로 라벨링 되어있고, 토픽으로 발행 된 메시지는 이를 구독하는 컨슈머 그룹 안에 한 컨슈머 인스턴스로 전달됩니다.
컨슈머 그룹 안에 인스턴스들은 다른 프로세스나 다른 기기에 속할 수도 있습니다.<br>

만약 모든 컨슈머 인스턴스들이 같은 컨슈머 그룹에 속해있다면, 이는 큐처럼 컨슈머들을 로드 밸런싱 합니다.<br>

만약 모든 컨슈머가 다른 컨슈머 그룹에 속해 있다면, 이는 모든 메시지를 모든 컨슈머에게 브로드캐스트 하는 발행-구독 모델처럼 동작합니다.<br>

하지만 일반적으로, 작은 컨슈머 그룹을 갖는 토픽들은
각 그룹은 확장성과 결함 허용성을 위해 많은 컨슈머들로 이루어져있습니다.
구독자는 하나의 프로세스 대신 컨슈머 클러스터라는 발행-구독의 의미과 다를바 없습니다.<br>

두 서버로 이루어진 카프카 클러스터는 4개의 파티션(P0-P3)을 두 개의 컨슈머 그룹으로 주관하고 있습니다.
컨슈머 그룹 A는 두개의 컨슈머 인스턴스를 가지고 있고 그룹 B는 4개를 가지고 있습니다.<br>

![consumer-groups](image/consumer-groups.png)

카프카는 기존의 메시징 시스템보다 강력한 순서 보장 기능을 가지고 있습니다.<br>

기존의 큐는 메시지를 서버에 온 순으로 가지고 있고, 만약 여러 컨슈머가 큐로부터 메시지를 받으려 한다면 서버는 메시지가 저장된 순서대로 보내줘야 합니다.
하지만, 서버가 순서대로 보내준다고 하더라도, 메시지는 컨슈머에 비동기적으로 전달되어 컨슈머마다 다른 순서로 도착할 것입니다.
이는 메시지를 순서가 병렬 처리환경에서는 흐트러질 수 있음을 의미합니다.
메시징 시스템은 종종 큐로부터 한 프로세스만 허용되는 "전용 컨슈머"를 사용해야 하는 경우가 있는데, 이 역시 병렬처리가 불가능 함을 의미합니다.<br>

카프카는 더 낫습니다.
토픽들 안에서는 파티션 간의 병렬처리가 가능하기 때문에, 카프카는 메시지의 순서을 보장하는 동시에 컨슈머 프로세스 풀 안에서의 로드 밸런싱역시 가능합니다.
이는 토픽 안의 파티션들을 컨슈머 그룹 속 컨슈머들에게 배정함으로써, 해당 그룹 안에서 한 파티션은 반드시 한 컨슈머에게만 읽혀질 수 있습니다.
이를 통해 한 컨슈머는 한 파티션의 리더(reader)가 되고, 파티션 속의 데이터를 순서대로 처리함을 보장할 수 있습니다.
시스템 안에는 많은 파티션이 존재하기 때문에 아직 컨슈머 인스턴스 간 로드밸런싱은 계속 됩니다.
하지만 컨슈머 그룹 안의 컨슈머 인스턴스는 파티션의 개수보다 많아질 수 없습니다.<br>

카프카는 파티션 안에서의 순서를 지킬 뿐, 같은 토픽이라도 다른 파티션 사이에서의 순서는 지켜지지 않습니다.
대부분의 어플리케이션에서는 데이터를 파티셔닝 하여 순서를 지키는 것 만으로도 충분합니다.
하지만 모든 메시지 사이의 순서를 보장되길 바란다면, 컨슈머 그룹에 하나의 컨슈머만 이를 처리하게 됨을 무릅쓰고라도 한 토픽에 하나의 파티션만 두면 됩니다.

### 보장
고수준의 카프카는 아래와 같은 보장을 지원합니다:

- 프로듀서에서 특정 파티션으로 보내진 메시지는 보내진 순서에 따라 파티션에 추가됩니다.
만약 메시지 M1이 메시지 M2를 보낸 프로듀서에 의해 보내졌고, M1이 먼저 보내졌다면, M1은 M2보다 낮은 오프셋을 갖게 되며 로그에 먼저 나타나게 됩니다.
- 컨슈머 인스턴스는 로그에 저장된 순서에 따라 메시지를 확인합니다.
- Replication factor의 값이 N일 때, N-1번의 서버 결함이 발생해도 로그에 커밋 된 어떤 메시지도 손실되지 않습니다.

더 자세한 내용은 디자인 섹션에 나와있습니다.

## 1.2 유즈케이스
아파치 카프카에 대한 몇 가지 유명한 유즈케이스가 있습니다.
관련 분야에 대한 전반적인 내용을 보려면, 이 [블로그 포스트](http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)를 보세요.

### 메시징
카프카는 전통적인 메시지 브로커의 대안으로 잘 작동합니다.
메시지 브로커는 다양한 목적(데이터 프로듀서로부터 프로세싱을 분리하기 위해, 아직 처리되지 않은 메시지를 버퍼에 넣어두기 위해, 등등)으로 사용되어 왔습니다.
다른 대부분의 메시징 시스템과 비교했을 때 카프카는 더 좋은 처리량, 내장된 파티셔닝, 복제, 결함 허용성과 같이 대규모 메시지 처리 어플리케이션으로서 좋은 솔루션을 제공합니다.<br>

우리의 실험에서 메시징 기능은 종종 상대적으로 낮은 처리량을 보였지만, 카프카는 낮은 엔드 투 엔드 레이턴시와 강한 내구성을 보장합니다.<br>

카프카는 ActiveMQ나 RabbitMQ와 같은 기존의 메시징 시스템과 비교해도 경쟁력 있는 시스템입니다.

### 웹사이트 액티비티 트래킹
카프카의 원래 용도는 실시간으로 발행-구독(publish-subscript model)되는 피드들을 유저 액티비티 트래킹 파이프라인으로 재 빌드하는 것이였습니다.
이는 (페이지 뷰, 서치 또는 유저들이 만드는 다른 활동들 같은)사이트 액티비니가 센트럴 토픽(한 엑티비티 유형 당 한 토픽)들로 발행됨을 의미합니다.
이러한 피드들은 실시간처리, 실시간 모니터링, 하둡 혹은 오프라인 데이터 웨어하우스 시스템과 같은 여러 범위의 구독을 위해 사용 가능합니다.<br>

종종 액티비티 트래킹은 사용자 페이지 뷰마다 발생되는 많은 양의 액티비티 메시지들로 인해 매우 큰 볼륨을 갖기도 합니다.

### 통계
생략...

### 로그 어그리게이션
많은 사람들이 카프카를 로그 어그리게이션 솔루션의 대안으로 사용합니다. 로그 어그리게이션은 그 처리를 위해 물리적인 로그파일을 여러 서버들로부터 하나의 중심(아마도 한 파일서버 혹은 HDFS)으로 모으는 것을 말합니다.
카프카는 파일들의 세부사항을 추상화하고 로그나 이벤트 데이터를 메시지 스트림으로 보다 명료하게 합니다.
이는 낮은 레이턴시의 프로세싱과 여러 데이터 소스, 분산 데이터 컨숨션을 가능하게 합니다.
Scribe나 Flume과 같은 로그 중심 시스템과 비교했을 때, 카프카는 같은 퀄리티의 퍼포먼스, 복제를 통한 더 강력한 내구성 보장, 더 낮은 엔드 투 엔드 레이턴시를 제공합니다.

### 스트림 프로세싱
생략...

### 이벤트 소싱
생략...

### 커밋 로그
생략...

## 1.3 퀵 스타트
이 튜토리얼은 당신이 카프카를 처음으로 시작하거나 카프카 혹은 주키퍼 데이터를 갖고 있지 않음을 가정하고 쓰여졌습니다.

### 스텝 1: 다운로드 코드
0.9.0.0 릴리즈를 [다운로드](https://www.apache.org/dyn/closer.cgi?path=/kafka/0.9.0.0/kafka_2.11-0.9.0.0.tgz) 하고 압출을 해제합니다.
```
> tar -xzf kafka_2.11-0.9.0.0.tgz
> cd kafka_2.11-0.9.0.0
```

### 스텝 2: 서버 시작
카프카는 주키퍼를 이용하기 때문에, 만약 주키퍼를 설치하지 않았다면 먼저 실행해야 합니다.
간편한 스크립트 패키지된 카프카를 이용하여 빠르게 싱글 노드 주키퍼 인스턴스를 얻을 수 있습니다.
```
> bin/zookeeper-server-start.sh config/zookeeper.properties
[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
...
```

이제 카프카 서버를 시작합니다:
```
> bin/kafka-server-start.sh config/server.properties
[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)
[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)
...
```

### 스텝 3: 토픽 생성
"test"라는 이름의 토픽을 하나의 파티션과 하나의 복제본을 갖도록 만들어 봅시다.
```
> bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
```

리스트 토픽 명령을 이용하여 토픽을 확인할 수 있습니다.
```
> bin/kafka-topics.sh --list --zookeeper localhost:2181
test
```

### 스텝 4: 메시지 전송
카프카는 파일이나 스탠다드 인풋으로 부터 얻은 입력을 커맨드라인 클라이언트로 동작합니다.
디폴트로 설정하면 각 라인이 개별적인 메시지로 전송 될 것입니다.<br>

프로듀서를 작동시킨 다음 서버에 보낼 메시지를 콘솔에 입력합니다.
```
> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
This is a message
This is another message
```

### 스텝 5: 컨슈머 시작
카프카 역시 메시지를 스탠다드 아웃풋으로 덤프아웃 할 커맨드라인 컨슈머가 있습니다.

```
> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
This is a message
This is another message
```

만약 위의 명령들이 다른 터미널에서 실행됐다면 프로듀서 터미널에 메시지를 입력하고 이를 컨슈머 터미널에서 볼 수 있습니다.<br>

모든 커맨드라인 툴들은 추가적인 옵션이 있습니다; 아무 인자 없이 명령을 실행시키면 해당 명령에 대한 자세한 정보들이 나타날 것입니다.

### 스텝 6: 멀티브로커 클러스터 설정
지금까지 우리는 하나의 브로커만을 실행시켰지만, 이는 그다지 재밌지 않습니다.
카프카에서는, 싱글 브로커는 그저 사이즈가 1인 클러스터이고, 몇개의 브로커 인스턴스들을 시작하는데 그다지 많은 차이가 없습니다.
하지만 그냥 해보세요, 우리의 클러스터를 노드 3개(아직은 모두 로컬머신 위에 있습니다)까지 확징시켜 봅시다.<br>

먼저 각 브로커들의 config 파일을을 만듭니다:
```
> cp config/server.properties config/server-1.properties
> cp config/server.properties config/server-2.properties
```
이제 저 파일들을 아래 프로퍼티와 같도록 설정합니다:
```
config/server-1.properties:
    broker.id=1
    port=9093
    log.dir=/tmp/kafka-logs-1

config/server-2.properties:
    broker.id=2
    port=9094
    log.dir=/tmp/kafka-logs-2
```

brocker.id 속성은 클러스터 안에서 각 노드가 갖는 유일하고 영구적인 이름입니다.
우리는 이 브로커들을 같은 머신 위에서 실행시키고 같은 포트에 등록시키거나 서로의 데이터를 덮어쓰도록 포트와 로그 디렉토리를 재정의해야 합니다.<br>
이미 주키퍼를 설치했고 싱글노드가 시작되었기 때문에, 남은 두개의 새 노드만 실행시키면 됩니다:
```
> bin/kafka-server-start.sh config/server-1.properties &
...
> bin/kafka-server-start.sh config/server-2.properties &
...
```

이제 레플리케이션 팩터를 3으로 설정하고 새로운 토픽을 만듭니다:
```
> bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic
```

하지만 이제 어떤 브로커가 클러스터안에서 무엇을 하는지 알 수 있을까요?
"describe topics" 명령을 실행시켜 확인해 보세요.
```
> bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
```

여기에 아웃풋에 대한 설명이 있습니다.
첫번째 라인은 모든 파티션의 요약을 보여주고, 각각 추가된 라인은 한 파티션에 대한 정보를 말합니다.
우리는 이 토픽에 대한 하나의 파티션만을 가지고 있기 때문에 한줄만 출력됩니다.
"leader"는 주어진 파티션에 대한 모든 읽기 및 쓰기 책임을 가지고 있습니다.
각 노드는 파티션의 비율에 따라 랜덤으로 리더가 될 수 있습니다.<br>
"replicas"는 노드들이 리더이거나 살아있는지와 관계없이 이 파티션의 로그를 복제하는 노드들의 리스트입니다.<br>
"isr"은 "in-sync" 레프리카의 집합입니다.
이는 리더로 뽑혔거나 지금 살아있는 레플리카 리스트의 부분집합입니다.<br>
이 예제에서는 노드 1이 이 토픽의 파티션에 대한 유일한 리더입니다.<br>
우리가 만든 원본 토픽들이 어디에 있는지 알기 위해 같은 명령을 실행할 수 있습니다:
```
> bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: test	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
```

토픽이 생성되었을 때 원본 토픽에 레플리카가 없고 서버 0이 우리 클러스터의 유일한 서버인 것은 그리 놀랄일은 아닙니다.<br>
우리의 새 토픽에 메시지들을 발행해 봅시다:
```
> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
...
my test message 1
my test message 2
^C
```

이제 메시지를 소비해봅시다.
```
> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
```

결함 허용성을 실험해봅시다.
리더로 동작하고 있는 브로커 1을 kill 해봅시다.
```
> ps | grep server-1.properties
7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/bin/java...
> kill -9 7564
```

리더는 슬레이브 중 하나로 이동했고 노드 1은 더이상 in-sync 레플리카 집합에 있지 않습니다:
```
> bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0
```

하지만 메시지들은 원래의 리더가 다운되더라도 여전히 소비가 가능합니다.
```
> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
^C
```

### 스텝 7: 카프카를 임포트/익스포트 데이터에 연결
콘솔에 데이터를 작성하고 콘솔로 응답 받는 것은 카프카를 처음 시작하기에 편리하지만, 아마 다른 소스나 다른 시스템에서 카프카로 데이터를 익스포트 하고 싶어질 것입니다.
다른 많은 시스템에 대해서는, 커스텀 통합 코드를 쓰는 것 대신에 카프카 커넥트를 사용해 데이터를 임포트 혹은 익스포트 해올 수 있습니다.
카프카 커넥트는 카프카에 데이터를 임포트하고 익스포트 할 수 있는 툴입니다.
이는 커넥터를 실행하는 확장 가능한 툴로, 외부 시스템과 상호작용할 수 있는 커스텀 로직을 구현하고 있습니다.
이 퀵스타트에서는 파일로부터 카프카 토픽으로 데이터를 임포트하고 카프카 토픽으로부터 파일로 익스포트하는 간단한 연결을 카프카 커넥트로 어떻게 실행하는지 볼 것입니다.<br>
먼저, 테스트할 시드 데이터를 만들며 시작합니다:
```
> echo -e "foo\nbar" > test.txt
```

그 다음, 우리는 두개의 커넥터를 스탠드얼론 모드, 즉 로컬에서 혼자서 실행되는, 전용 프로세스로 시작합니다.
우리는 3개의 configuration 파일을 파라미터로 제공합니다.
첫 째는 항상 카프카 커넥트 프로세스의 configuration 이고, 연결할 카프카 브로커와 데이터 직렬화 포멧과 같은 일반적인 configuration 입니다.
남은 configuration 파일은 각각 생성할 커넥터를 특정짓습니다.
```
> bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
```

카프카에 포함 된 이러한 샘플 configuration 파일은 이전에 시작한 디폴트 로컬 클러스터 configuration으로 사용되며 두개의 커넥터를 생성합니다: 첫번째는 인풋파일로부터 라인을 읽고 각 카프카 토픽에 프로듀스하는 소스 커넥터이고, 두번재는 카프카 토픽으로부터 메시지를 읽고 아웃풋 파일의 각 라인으로 프로듀스하는 싱크 커넥터입니다.
시작하는 동안 몇개의 로그메시지를 볼 수 있으며, 그 중 몇 가지는 커넥터가 인스턴스화 되었음을 나타냅니다.
카프카 프로세스가 한번 시작되면, 소스 커넥터는 아래파일로부터 라인을 읽고
```
test.txt
```

그 토픽으로 프로듀싱 하며
```
connect-test
```

, 싱크 커넥터는 토픽으로부터 메시지를 일고
```
connect-test
```

이것을 파일에 씁니다
```
test.sink.txt
```

. 아웃풋 파일의 내용을 확인함으로써 전체 파이프라인을 통해 데이터가 전달 되었음을 확인할 수 있습니다:
```
> cat test.sink.txt
foo
bar
```

데이터가 카프카 토픽안에 저장 되었으므로
```
connect-test
```

, 토픽 안의 데이터를 보기 위해 콘솔 컨슈머를 실행시킬 수도 있습니다(혹은 커스텀 컨슈머 코드를 사용할 수 있습니다):
```
> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic connect-test --from-beginning
{"schema":{"type":"string","optional":false},"payload":"foo"}
{"schema":{"type":"string","optional":false},"payload":"bar"}
...
```

커넥터들은 계속헤서 데이터를 처리하기 때문에, 데이터를 파일에 추가하고 파이프 라인을 통해 데이터가 이동하는 것을 확인할 수 있습니다:
```
> echo "Another line" >> test.txt
```

콘솔 컨슈머 아웃풋과 싱크 파일에서 그 라인을 볼 수 있습니다.


## 1.4 에코시스템
정말 많은 툴들이 카프카의 메인 배포판 외부에서 통합될 수 있습니다.
[에코시스템 페이지](https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem)는 스트림 프로세싱 시스템, 하둡 인테그레이션, 모니터링, 배포툴을 포함한 많은 것들이 정리되어 있습니다.


## 1.5 이전 버전에서의 업그레이드
업그레이드는 생략...
