# **Kafka 0.9.0 Documentation**
---
# 4 디자인
## 4.1 동기
우리는 광범위한 유즈케이스를 고려하여, 실시간 데이터 피드를 처리할 수 있는 통합 플랫폼으로 카프카를 디자인했습니다.<br>

실시간 로그 어그리게이션과 같이 큰 볼륨을 갖는 실시간 이벤트 스트림을 지원하기 위해 카프카는 높은 처리량을 가져야 했으며, 오프라인 시스템으로부터의 주기적인 데이터 부하를 지원하기 위해 카프카는 큰 데이터 백로그를 잘 다룰 필요가 있었습니다.
또한 기존의 메시징 유즈케이스를 여전히 지원하기 위해서 낮은 레이턴시 딜리버리를 가져야 했습니다.<br>

우리는 새로운 피드를 만들기위해 파티셔닝, 분산 시스템, 실시간 처리를 지원하기를 원했습니다.
이렇게 우리의 피티셔닝 컨슈머 모델이 시작되었습니다.<br>

결국 데이터 시스템으로 스트림이 들어오는 동안, 머신의 문제가 발생할 위험 속에서도 결함 허용성을 보장할 수 있었습니다.<br>

이러한 기능을 지원함으써 몇가지 독득한 디자인 요소와, 기존의 메시징 시스템보다 데이터베이스 로그에 더 가까운 시스템을 디자인할 수 있었습니다.

## 4.2 지속성
**파일시스템을 두려워 하지 마세요** <br>

카프카는 메시지를 캐싱하고 저장하기위해 파일시스템 위에 올라가 있습니다.
보통 "디스크는 느리다"라는 생각이 있기 때문에, 디스크와 같이 지속성이 있는 구조는 경쟁력있는 성능을 보여줄 수 있다는 말에 사람들은 회의적입니다.
사실 디스크는 어떻게 사용하는가에 따라 사람들이 생각하는 것 보다 느리기도하고 빠르기도 합니다:
그리고 제대로 디자인 된 디스크 구조는 네트워크만큼 빠른 속도를 가집니다.<br>

디스크 성능의 중요한 사실은 하드디스크의 처리량은 지난 십년 동안 Disk Seek Latency로부터 증가해왔다는 것입니다.
그 결과, 6개의 7200rpm SATA RAID-5 어레이 [JBOD](http://en.wikipedia.org/wiki/Non-RAID_drive_architectures) 설정에서 linear write에서는 600MB/sec인 반면, random write 에서는 100k/sec정도의 성능을 보여줍니다.
linear read와 write는 모든 사용패턴에 대해서 거의 예측 가능하고, 운영체제에 의해 매우 최적화 되어있습니다.
현대 운영체제는 데이터를 블록에서 미리 읽거나, 논리적인 write를 하나의 물리적인 write로 그룹핑 하는, read-ahead와 write-behind 기능을 제공합니다.
이 이슈에 관한 더 많은 논의는 [ACM Queue article](http://queue.acm.org/detail.cfm?id=1563874)에서 볼 수 있습니다;
여기서는 [시퀀셜 디스크 억세스가 랜덤 메모리 억세스보다 더 빠를 수 있는 상황들](http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg)에 대한 자료를 볼 수 있습니다.<br>

게다가 우리는 카프카를 JVM위에 올렸고, 자바 메모리 사용에 대해 공부해본 적 있는 사람은 아래 두가지 사실을 알 것입니다:

- 1. 객체들의 메모리 오버헤드는 매우 높습니다. 종종 (최악의 경우)저장된 데이터의 두배가 되기도 합니다
- 2. 자바의 가비지컬렉션은 힙영역의 데이터가 증가함에 따라 점점 성가시게 느려집니다.

이 사실에 따르면 파일시스템과 페이지 캐시를 이용하는 것이 메모리 캐시나 다른 구조를 유지하는 것보다 더 유리함을 알 수 있습니다 - 우리는 모든 가용 메모리에 자동으로 접근함으로써 적어도 두배의 가용 캐시를 가질 수 있고, 같은 이유로 간단한 바이트 스트럭처로 저장함으로써 개별의 객체로 저장하는 것보다 두배 낫습니다.
이렇게 하면 GC의 패널티 없이 32GB 머신에서 28-30GB까지의 캐시가 발생합니다.
게다가 이 캐시는 서비스가 재시작되어도 warm한 상태인 반면, in-process 캐시는 메모리에 리빌트 하거나 cold한 상태(초기 성능이 매우 안좋습니다)에서 시작해야 합니다.
또한 이는 캐시와 운영체제의 파일시스템사이의 응집성을 유지하기위한 모든 로직의 코드를 매우 간단하게 만들며, 일회성의 in-process 보다 더 효율적이고 정확한 편입니다.
만약 당신의 디스크 사용 경향이 linear read라면, read-ahead는 디스크 유용한 데이터들을 캐시에 효과적으로 채워넣을 수 있습니다.<br>

이러한 방식은, 메모리 부족의 위험이 있는 인메모리에 방식(데이터를 메모리에 보관했다가 디스크에 옮기는 방식)보다, 훨씬 더 간단한 디자인을 제안합니다.
모든 데이터는 메모리에서 디스크로 플러싱 하는 과정 없이 즉시 파일시스템에 영구적인 로그로 써집니다.
이는 커널의 페이지캐시로 바로 옮겨지는 것을 의미합니다.<br>

이러한 페이지캐시 중심적 디자인 스타일은 Varnish에 실린 [이 글](http://varnish.projects.linpro.no/wiki/ArchitectNotes)에 설명되어 있습니다.<br>

**상수시간 충족** <br>

메시지 시스템에서 지속적인 데이터 구조는 메시지의 메타데이터를 갖고 있는 이진트리나 랜덤엑세스를 위한 자료구조와 관련된 컨슈머 큐입니다.
이진트리는 매우 다재다능하여, 메시징 시스템에서 transactional 하거나 non-transactional한 시멘틱 모두를 가능하게 합니다.
하지만 이진트리는 O(log N)의 성능으로 꽤 높은 비용이 듭니다.
보통 O(log N)은 상수시간과 동일하게 여겨지기도 하지만, 디스크에서는 사실이 아닙니다.
디스크 탐색은 10ms 정도 걸리며, 하나의 디스크는 한번에 하나의 탐색만 가능하기 때문에 병렬적인 탐색이 제한됩니다.
따라서 몇번의 디스크 탐색도 매우 높은 오버헤드를 일으킵니다.
스토리지 시스템에서 캐싱과 물리적인 디스크 탐색이 병행되면서, 트리구조의 관측된 성능은 한정된 캐시에서 데이터가 증가함에 따라 매우 선형적입니다 -- 데이터를 두배 늘리면 성능은 두배 느려집니다.<br>

직관적으로, 로깅 솔루션의 경우에서 지속적인 큐는 파일에서 데이터를 읽거나 덧붙이는 방식입니다.
이 구조는 모든 연산이 O(1) 안에 이루어지고 읽기 쓰기과정에서 서로를 블록하지 않는 장점이 있습니다.
이는 데이터의 사이즈와 성능의 관계를 완전히 분리하기 때문에, 이제 한 서버에 저렴한 low-rotational speed 1+TB SATA drive를 이용할 수 있는 장점이 있습니다.
디스크의 탐색 성능이 나쁘긴 하지만, 큰 데이터를 읽고 쓰는데는 그래도 쓸만한 성능을 가지고있어 1/3 가격으로 3배의 가용성을 얻을 수 있습니다.<br>

아무런 성능저하 없이 디스크 공간에 접근한다는 것은 기존 메시징 시스템에서는 찾을 수 없던 기능을 제공함을 의미합니다.
예를 들어 카프카에서는, 메시지가 소비되는 즉시 이를 삭제하는 대신에, 메시지를 장기간(주 단위)로 보관할 수 있습니다.
이는 컨슈머가 메시지를 처리하는데 큰 유연성을 가질 수 있도록 합니다.

## 4.3 효율성

우리는 효율에 대해서 매우 큰 노력을 기울였습니다.
가장 중요하게 여겨졌던 유즈케이스 중 하나는 웹 액티비티 데이터를 다루는 것이였고, 이는 보통 매우 큰 볼륨을 갖고 있습니다: 각 페이지 뷰는 수십번의 쓰기를 발생시킵니다.
게다가 우리는 발행 된 각 메시지가 적어도 하나 이상의 컨슈머에 의해 읽혀진다고 가정했기 때문에, 우리는 메시지 소비에 필요한 비용을 가능한 한 값싸게 만들기 위해 고군분투했습니다.<br>

또한 우리가 기존의 비슷한 몇 개의 시스템들을 빌드하고 돌려보면서 얻은 경험에 의하면, 효율성은 멀티 테넌트 운영에 있어 매우 중요한 키입니다.
만약 다운스트림 인프라스트럭처 서비스가 어플리케이션이 일으킨 작은 범프에 의해 병목현상을 일으킨다면, 이런 작은 변화는 많은 문제들을 일으킬 것입니다.
우리는 빠르게 작동함으로써 어플리케이션이 인프라스트럭처 전에 부하를 덜어낼 수 있도록 돕습니다.
이는 중앙집중형 클러스터가 거의 매일 같은 패턴의 변화를 일으키는 수십 수백개의 어플리케이션을 서비스 하는 경우에 특히 더 중요합니다.<br>

우리는 디스크 효율성에 대해서 이전 섹션에서 얘기했습니다.
좋지 못한 디스크 접근 패턴의 문제가 해결되고 나면, 이제 해결해야 할 남은 비효율은 지나치게 많은 작은 I/O와 과도한 바이트 복사 두가지입니다.<br>

작은 I/O 문제는 클라이언트와 서버 사이, 서버 내부의 지속적인 연산에서 발생합니다.<br>

이를 피하기 위한 우리의 프로토콜은, 메시지를 그룹으로 묶는 "메시지 세트"라는 추상화를 중심으로 만들어졌습니다.
이는 메시지를 그룹으로 묶도록 요청함으로써 메시지를 즉각적으로 보낼때 보다 네트워크 라운드트립의 오버헤드를 경감시킵니다.
서버는 메시지들이 하나의 청크(메시지 덩어리)로 합쳐 한번에 보내고, 컨슈머는 커다란 메시지 청크를 한번에 페치합니다.<br>

이 작은 최적화는 수십배의 속도 향상을 불러옵니다.
배치(일괄처리)는 매우 큰 네트워크 패킷발생과, 큰 시퀀셜 디스크 접근, 인접한 메모리 블록접근을 일으키는데, 대부분의 것들은 카프카를 통해 random write에서 linear write로 바뀌어 컨슈머에게 전달됩니다.<br>

또 다른 비효율성은 바이트 복사에 관한 것입니다.
이는 낮은 메시지 레이트에선 그다지 문제가 되지 않지만, 부하가 집중될 때는 매우 중요합니다.
이를 피하기 위해 우리는 프로듀서와 브로커, 컨슈머 사이에서 공유되는 일반화된 이진 메시지 포맷을 도입하였습니다(이 덕분에 데이터 청크가 이들 사이에서 별도의 수정 없이 전달 될 수 있습니다).<br>

브로커에 유지되는 메시지 로그는 파일들의 디렉토리 그 자체이며, 이들은 디스크에 특정 포멧(프로듀서와 컨슈머에서도 동일하게 사용하는)으로 쓰는 과정, 즉 메시지 세트 시퀀스에 의해 디스크에 채워집니다.
이들을 같은 포멧으로 유지하는 것은 "지속적인 로그 청크의 네트워크 전송"이라는 가장 중요한 작업의 최적화를 하기 위함입니다.
현대 유닉스는 데이터를 페이지캐시에서 소켓으로 전송하는 매우 최적화된 코드패스를 제공합니다; 리눅스는 이를 [sendfile 시스템 콜](http://man7.org/linux/man-pages/man2/sendfile.2.html)을 통해 합니다.<br>

sendfile의 효과를 이해하기 위해서는, 데이터를 파일에서 소켓으로 전송하는 일반적인 데이터 패스를 이해하는 것이 중요합니다.

- 1. 운영체제는 데이터를 디스크에서 커널영역의 페이지캐시로 옮긴다
- 2. 어플리케이션은 데이터를 커널영역에서 유저영역 버퍼로 옮긴다
- 3. 어플리케이션은 데이터를 읽어 커널영역의 소켓 버퍼에 쓴다
- 4. 운영체제는 소켓버퍼에서 데이터를 읽어 네트워크로 데이터를 발송하는 NIC 버퍼에 복사한다

이것은 아주 비효율적인데, 두번의 시스템 콜에서 네번의 복사가 발생하기 때문입니다.
sendfile을 이용하면, 운영체제가 데이터를 페이지캐시에서 네트워크로 직접 보내기 때문에 재복사를 피할 수 있습니다.
따라서 이 패스를 최적화하여, NIC 버퍼로 한번의 복사만이 필요하게 됩니다.<br>

우리는 하나의 토픽에 여러 컨슈머가 붙는 일반적인 유즈케이스를 예상했습니다.
위의 zero-copy 최적화를 통해 데이터는 패이지케시로 정확히 한번 복사되고, 인메모리의 메시지가 커널역역에 여러번 카피되는 대신에 패이지캐시에서 재사용 가능합니다.
이는 네트워크 커넥션 속도의 최대치와 가까운 속도로 메시지가 소비되도록 합니다.<br>

대부분의 컨슈머들이 캐치업만하는 카프카 클러스터에서는 데이터를 캐시에서만 제공하는데, 이때 페이지캐시와 sendfile의 조합은 이러한 상황에서 어떠한 디스크 읽기도 일어나지 않도록 합니다.<br>

자바에서의 sendfile과 zero-copy지원에 대해서 더 알아보려면 [이 글](http://www.ibm.com/developerworks/linux/library/j-zerocopy)을 읽어보세요.<br>

**엔드 투 엔드 배치 압축**<br>

병목현상의 몇가지 케이스는 CPU나 디스크가 아닌 네트워크 대역폭인 경우입니다.
이는 특히 WAN을 커버하는 데이터 센터들 사이에서 메시지를 주고받을 필요가 있는 데이터 파이프라인에서 더 그렇습니다.
당연히도 유저는 카프카의 지원 없이 메시지들을 압축할 수 있지만, 이는 같은 타입의 메시지들 간의 반복으로 인한 중복으로 매우 낮은 압축률로 이어집니다 (예를 들면, JSON에서 필드 이름 혹은 웹 로그의 유저 에이전트, 일반 스트링 값들).
압축은 여러 메시지를 한번에 압축하는 것이 메시지를 각각 압축하는 것 보다 더 효율적입니다.<br>

카프카는 재귀적인 메세지 세트을 지원함으로써 이를 지원합니다.
메세지 배치는 이러한 형태로 함께 묶여서 압축되고 서버로 보내질 수 있습니다.
이 메시지 배치는 압축된 형태로 쓰여지고 압축된 채로 유지될 수 있으며 컨슈머에 의해서만 압축이 해제될 수 있습니다.<br>

카프카는 GZIP과 Snappy 압축 프로토콜을 지원합니다.
압축과 관련 된 자세한 내용은 [여기](https://cwiki.apache.org/confluence/display/KAFKA/Compression)에서 볼 수 있습니다.

## 4.4 프로듀서

**로드 밸런싱**

프로듀서는 데이터를 해당 파티션의 리더인 브로커에게 중간 라우팅 과정 없이 직접 보냅니다.
프로듀서가 이렇게 동작하기 위해서는 지금 어떤 서버가 살아있고 해당 토픽 파티션의 리더가 어디있는지에 대해 모든 카프카 노드가 주어진 시간 내에 응답할 수 있어야 합니다.<br>

클라아인트는 메시지를 어떤 파티션으로 보낼지 조정할 수 있습니다.
이는 랜덤으로 이뤄지는데, 일종의 랜덤로드 밸런싱 혹은, 시멘틱 파티셔닝 함수를 통해 구현됩니다.
우리는 유저가 파티션 해시를 이용해 파티션 키를 지정하게함으로써 시멘틱 파티셔닝 인터페이스를 제공합니다(또한 필요하다면 파티션 함수를 오버라이드해서 사용할 수 있습니다).
예를들어 만약 선택된 키가 유저 ID 라면, 유저에게 주어진 모든 데이터는 같은 파티션으로 보내질 것입니다.
이는 컨슈머가 그들의 소비에 대해 locality assumptions를 가능하게 합니다.
이러한 파티셔닝 스타일은 컨슈머가 locality-sensitive processing을 할 수 있도록 명시적으로 디자인 되어있습니다.<br>

**비동기적인 발송**<br>

배치는 효율성을 위한 큰 축 중 하나인 동시에, 카프카가 데이터를 메모리에 축적하고 이를 큰 배치로 만들어 한번의 요청으로 처리할 수 있도록 합니다.
배치는 설정을 통해 지정된 개수의 메시지 이상으로 축적하지 않게하고 특정 레이턴시 이상으로 대기하지 않도록 할 수 있습니다.
이는 서버가 몇 바이트 이상의 축적을 막고 큰 단위의 I/O 작업을 하도록 합니다.
이러한 버퍼링은 설정이 가능해서, 더 나은 처리량을 위해 약간의 추가적인 레이턴시를 줄 수 있습니다.<br>

이러한 [설정](http://kafka.apache.org/documentation.html#producerconfigs)과 관련 프로듀서 [API](http://kafka.apache.org/082/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html)는 다른 문서에서 찾아볼 수 있습니다.

## 4.5 컨슈머

카프카 컨슈머는 원하는 파티션을 리딩하고 있는 프로커에게 "페치" 요청을 함으로써 동적합니다.
컨슈머는 각 요청마다 로그 안에서의 오프셋을 명시하고 해당 위치로부터 시작하는 로그의 청크를 돌려받습니다.
따라서 컨슈머는 이 위치를 컨트롤할 수 있으며 필요할경우 해당 데이터를 다시 처리하기 위해 위치를 되돌릴 수 있습니다.

**푸시 vs. 풀**

우리가 처음에 고려했던 질문은, 컨슈머가 브로커로부터 데이터를 풀 받을 것인지 브로커가 컨슈머에게 데이터를 푸시할 것인지 였습니다.
이와 관련해서 카프카는 더 전동적인 디자인을 따르는데, 다른 대부분의 메시징 시스템이 그러하듯, 프로듀서는 브로커로 데이터를 푸시하고 컨슈머는 브로커로부터 데이터를 풀 받는 것입니다.
Scribe와 Apache Flume과 같은 몇몇 로깅 중심의 시스템은 데이터가 다운스트림으로 푸시되는 방식의 매우 다른방식의 푸시기반 패스를 따릅니다.
두가지 접근에는 모두 장점과 단점이 있습니다.
하지만, 푸시 기반의 시스템은 브로커가 전송되는 데이터의 속도를 컨트롤하기 때문에 다양한 컨슈머에 대응하기 어렵습니다.
컨슈머의 일반적인 목표는 가능한 한 최대의 속도로 소비하는 것이지만, 안타깝게도, 푸시시스템에서 이는 소비속도가 생산속도를 따라가지 못할 때 컨슈머가 압도되는 경향이 있음을 의미합니다(서비스 거부 공격).
풀 기반의 시스템은 단순히 컨슈머가 뒤쳐졌다가 가능할 때 다시 따라잡는 방식의 보다 좋은 특징을 가지고 있습니다.
컨슈머의 전송속도를 완전이 활용하는것은 보기보다 난이도가 높긴 하지만, 이러한 문제는 컨슈머가 압도되었음을 지시하는 일종의 백오프 프로토콜을 통해 완화될 수 있습니다.
앞서 빌딩 시스템에 대한 시도들은 우리가 좀 더 전통적인 풀 모델로 가게끔 하였습니다.

풀 기반 시스템의 또다른 이점은 다른 컨슈머에게 보내진 공격적인 배치에 대해 자기 자신을 빌려줄 수 있다는 것입니다.
푸시 기반 시스템은 요청을 즉시 보낼것인지 데이터를 좀 더 축적함으로써 어떤 다운스트림 컨슈머가 이를 즉시 처리할 수 있는지 모르는 상태에서 보낼 것인지 결정해야만 합니다.
만약 낮은 레이턴시를 위해 조정한다면, 메시지가 버퍼링 되지 않도록 하나씩 전송하는 결과를 불러올 것이고, 이는 비효율적일 것입니다.
풀 기반 디자인은 컨슈머가 항상 로그 위치 이후의 가능한 모든 메시지를 풀하도록 되어있습니다.
따라서 불필요한 레이턴시 없이 배치를 최적화할 수 있습니다.

The deficiency of a naive pull-based system is that if the broker has no data the consumer may end up polling in a tight loop, effectively busy-waiting for data to arrive.
To avoid this we have parameters in our pull request that allow the consumer request to block in a "long poll" waiting until data arrives (and optionally waiting until a given number of bytes is available to ensure large transfer sizes).

당신은 끝에서 끝까지 풀만 가능한 디자인을 상상할 수 있었을 것입니다.
프로듀서는 로컬 로그에 쓰고, 브로커는 컨슈머가 이를 풀링 받도록 풀링하는 것입니다.
이런 "store-and-forward"과 유사한 타입의 프로듀서는 종종 제안되기도 합니다.
이는 아주 흥미롭지만 우리는 수천개의 프로듀서를 갖는 우리의 유즈케이스에는 매우 부적절하다고 느꼈습니다.
규모있는 지속적인 데이터 시스템을 운영해본 우리의 경험에 비추어 볼 때, 많은 어플리케이션에 걸쳐 수천개의 디스크를 갖고 있는 시스템은 안정적이지 못하며 이는 운영에 악몽으로 작용할 것이었습니다.
실제로 우리는 프로듀서의 지속성 없이 큰 규모에서 강력한 SLAs로 파이프라인을 작동시킬 수 있음을 알게 되었습니다.

**컨슈머 포지션**

어떤 것이 소비되었는지 트래킹 하는것은 놀랍게도 메시징시스템의 주요한 성능 포인트중의 하나입니다.
대부분의 메시징 시스템은 어떤 메시지가 브로커에서 소비되었는지에 대한 메타데이터를 유지합니다.
즉, 메시지가 컨슈머에 보내지면, 브로커가 이 사실을 로컬에 즉시 레코드하거나 컨슈머로부터의 응답을 기다립니다.
이는 꽤 직관적인 선택인데, 싱글머신 서버에게는 이 상태가 어디로 갈지 명확하지 않습니다.
확장하기 어려운 기존의 많은 메시징 시스템에서 이 데이터구조들을 스토리지로 사용하기 때문에, 이 역시 하나의 실용적인 선택입니다. --왜냐하면 브로커는 어떤 것이 소비되었는지 알고 즉시 삭제하여 데이터 크기를 작게 유지합니다.

What is perhaps not obvious is that getting the broker and consumer to come into agreement about what has been consumed is not a trivial problem.
무엇이 소비되었는지에 대한 동의  사소한 문제가 아닙니다.
만약 브로커가 소비된 메시지를 매번 즉시 기록하고 네트워크를 통해 전송하고 나서, 만약 컨슈머가 (크래시나 요청 타임아웃 따위로 인해)그 메시지를 처리하는데 실패하면 그 메시지는 유실될 것입니다.
이 문제를 해결하기 위해, 많은 메시징시스템들은 보낸 메시지들이 소비되지 않았음을 명시하는 응답을 추가했습니다; 브로커는 메시지가 소비됐는지 레코드하기 위해 컨슈머로부터 특정 응답을 기다립니다.
이 전략은 메시지가 유실되는 문제를 고칠 수 있었지만, 새로운 문제를 만들었습니다.
첫째로, 만약 컨슈머가 그 메시지를 처리했지만 응답을 보내기 전에 실패하면, 메시지는 두번 소비될 것입니다.
두번째 문제는 주변성능인데, 지금 브로커는 반드시 모든 싱글 메시지에 대해 여러 상태를를 유지해야 합니다(first to lock it so it is not given out a second time, and then to mark it as permanently consumed so that it can be removed).
보내졌지만 응답되지 않은 메시지에 대한 것처럼, 까다로운 문제는 반드시 다뤄져야 합니다.

카프카는 이를 다르게 처리합니다. 우리 토픽은 완전히 순서가 정해진 파티션들의 집합으로 나눠지고, 각각은 어느때이든지 한 컨슈머에의해 소비됩니다.
이는 각 파티션에 컨슈머 포지션은 그냥 하나의 정수값이며, 다음 소비될 메시지의 오프셋을 의미합니다.
이는 각 파티션마다 어떤 것이 소비되었는지에 대한 상태를 하나의 숫자로 작게 만듭니다.
이 상태는 주기적으로 체크됩니다. 이는 메시지 응답의 비용을 매우 저렴하게 합니다.

여기에는 부수적인 이득이 있습니다.
컨슈머는 이미 지나간 오프셋을 되돌린다음에 데이터를 다시 소비할 수 있습니다.
이는 일반적인 큐에서는 위반되는 행동이지만, 많은 컨슈머들에게는 필수적인 기능이 되었습니다.
예를 들어, 만약 컨슈머 코드에 버그가 있고 이미 몇몇 메시지가 소비된 다음에 발견되었다면, 컨슈머는 버그를 고친상태에서 그 메시지들을 다시 소비할 수 있습니다.

**오프라인 데이터 부하**

Scalable persistence allows for the possibility of consumers that only periodically consume such as batch data loads that periodically bulk-load data into an offline system such as Hadoop or a relational data warehouse.
In the case of Hadoop we parallelize the data load by splitting the load over individual map tasks, one for each node/topic/partition combination, allowing full parallelism in the loading. Hadoop provides the task management, and tasks which fail can restart without danger of duplicate data—they simply restart from their original position.
